{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b46fe97-2bb7-445a-99cc-d27e86846ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "import torch\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import init\n",
    "import math\n",
    "from math import gcd\n",
    "from math import sqrt\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class PHMLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, in_features, out_features,n=2):\n",
    "    super(PHMLayer, self).__init__()\n",
    "    self.n = n\n",
    "    self.in_features = in_features\n",
    "    self.out_features = out_features\n",
    "\n",
    "    self.bias = Parameter(torch.Tensor(out_features))\n",
    "\n",
    "    self.a = torch.zeros((n, n, n))\n",
    "    self.a = Parameter(torch.nn.init.xavier_uniform_(self.a))\n",
    "\n",
    "    self.s = torch.zeros((n, self.out_features//n, self.in_features//n)) \n",
    "    self.s = Parameter(torch.nn.init.xavier_uniform_(self.s))\n",
    "\n",
    "    self.weight = torch.zeros((self.out_features, self.in_features))\n",
    "\n",
    "    fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "    bound = 1 / math.sqrt(fan_in)\n",
    "    init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "  def kronecker_product1(self, a, b):\n",
    "\n",
    "    siz1 = torch.Size(torch.tensor(a.shape[-2:]) * torch.tensor(b.shape[-2:]))\n",
    "    \n",
    "    res = a.unsqueeze(-1).unsqueeze(-3) * b.unsqueeze(-2).unsqueeze(-4)\n",
    "    siz0 = res.shape[:-4]\n",
    "    out = res.reshape(siz0 + siz1)\n",
    "\n",
    "    return out\n",
    "\n",
    "  def forward(self, input: Tensor) -> Tensor:\n",
    "    self.weight = torch.sum(self.kronecker_product1(self.a, self.s), dim=0)\n",
    "\n",
    "    input = input.type(dtype=self.weight.type())\n",
    "\n",
    "      \n",
    "    return F.linear(input, weight=self.weight, bias=self.bias)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "    return 'in_features={}, out_features={}, bias={}'.format(\n",
    "      self.in_features, self.out_features, self.bias is not None)\n",
    "    \n",
    "  def reset_parameters(self) -> None:\n",
    "    init.kaiming_uniform_(self.a, a=math.sqrt(5))\n",
    "    init.kaiming_uniform_(self.s, a=math.sqrt(5))\n",
    "    fan_in, _ = init._calculate_fan_in_and_fan_out(self.placeholder)\n",
    "    bound = 1 / math.sqrt(fan_in)\n",
    "    init.uniform_(self.bias, -bound, bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a600dd85-6b3c-430e-9f9d-5cbf0f7936de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import RobertaTokenizer,RobertaModel\n",
    "class Model_Classifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_labels, dropout):\n",
    "        super(Model_Classifier, self).__init__()\n",
    "        # Instantiate BERT model\n",
    "        self.bert = RobertaModel.from_pretrained('roberta-large')\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = dropout\n",
    "        self.linear = nn.Linear(self.embedding_dim,self.num_labels)\n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "\n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state = outputs[0][:, 0, :]\n",
    "        \n",
    "        logits = self.linear(last_hidden_state)\n",
    "\n",
    "\n",
    "        #logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits, last_hidden_state\n",
    "class QModel_Classifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_labels, dropout,feature_remove_max= False):\n",
    "        super(QModel_Classifier, self).__init__()\n",
    "        # Instantiate BERT model\n",
    "        self.bert = RobertaModel.from_pretrained('roberta-large')\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = dropout\n",
    "\n",
    "        \n",
    "        divisors = sorted(self.cf(embedding_dim,hidden_dim))\n",
    "        divisors1 = sorted(self.cf(hidden_dim,num_labels))\n",
    "        common_divisors = sorted(set(divisors1) & set(divisors))\n",
    "        if(feature_remove_max == True):\n",
    "            self.n = common_divisors[-1]\n",
    "        else :\n",
    "            self.n = common_divisors[0]\n",
    "        \n",
    "        self.linear = PHMLayer(self.embedding_dim, self.hidden_dim,self.n)\n",
    "\n",
    "\n",
    "    def cf(self,num1,num2):\n",
    "            n=[]\n",
    "            g=gcd(num1, num2)\n",
    "            for i in range(1, int(sqrt(g))+1):\n",
    "                if g%i==0:\n",
    "                    n.append(i)\n",
    "                    if g!=i*i:\n",
    "                        n.append(int(g/i))\n",
    "            return n\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        #print(last_hidden_state_cls.shape)\n",
    "        logits = self.linear(last_hidden_state_cls)\n",
    "\n",
    "        \n",
    "\n",
    "        #print(logits.shape)\n",
    "        # Feed input to classifier to compute logits\n",
    "        #logits = self.classifier(last_hidden_state_cls)\n",
    "        \n",
    "        return logits, last_hidden_state_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19000543-312b-497a-b0c3-3656905a5256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model,embedding=1024,hidden = 16,num_classes = 2):\n",
    "    \"\"\"Initialize the Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    model_classifier = model(embedding, hidden, num_classes, dropout=0.1)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    model_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(model_classifier.parameters(),\n",
    "                      lr=1e-5,  # Default learning rate\n",
    "                      eps=1e-8  # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    return model_classifier, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2044eae5-e230-495a-96b9-6c19719921e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "class config():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(config, self).__init__()\n",
    "        self.data = None\n",
    "        self.bert_model = 'roberta-large'\n",
    "        self.num_labels = 0\n",
    "        self.epoch = 20\n",
    "        self.k_spt = 5\n",
    "        self.k_qry = 2\n",
    "        self.outer_batch_size = 2\n",
    "        self.inner_batch_size = 12\n",
    "        self.outer_update_lr = 5e-5\n",
    "        self.inner_update_lr = 5e-5\n",
    "        self.inner_update_step = 10\n",
    "        self.inner_update_step_eval = 40\n",
    "        self.num_task_train = 500\n",
    "        self.num_task_test = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1369c3a-69ff-40fc-af6c-7cadcbed51aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from math import gcd\n",
    "from math import sqrt\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "class Learner(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta Learner\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(Learner, self).__init__()\n",
    "        \n",
    "        self.num_labels = args.num_labels\n",
    "        self.outer_batch_size = args.outer_batch_size\n",
    "        self.inner_batch_size = args.inner_batch_size\n",
    "        self.outer_update_lr  = args.outer_update_lr\n",
    "        self.inner_update_lr  = args.inner_update_lr\n",
    "        self.inner_update_step = args.inner_update_step\n",
    "        self.inner_update_step_eval = args.inner_update_step_eval\n",
    "        self.bert_model = args.bert_model\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.model , self.outer_optimizer = initialize_model(QModel_Classifier,1024,16,self.num_labels)\n",
    "\n",
    "        \n",
    "        self.model.train()\n",
    "\n",
    "    def forward(self, batch_tasks, training = True):\n",
    "        \"\"\"\n",
    "        batch = [(support TensorDataset, query TensorDataset),\n",
    "                 (support TensorDataset, query TensorDataset),\n",
    "                 (support TensorDataset, query TensorDataset),\n",
    "                 (support TensorDataset, query TensorDataset)]\n",
    "        \n",
    "        # support = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)\n",
    "        \"\"\"\n",
    "        task_accs = []\n",
    "        task_loss = []\n",
    "        sum_gradients = []\n",
    "        num_task = len(batch_tasks)\n",
    "        num_inner_update_step = self.inner_update_step if training else self.inner_update_step_eval\n",
    "\n",
    "        for task_id, task in enumerate(batch_tasks):\n",
    "            support = task[0]\n",
    "            query   = task[1]\n",
    "            \n",
    "            fast_model = deepcopy(self.model)\n",
    "            fast_model.to(self.device)\n",
    "            support_dataloader = DataLoader(support, sampler=RandomSampler(support),\n",
    "                                            batch_size=self.inner_batch_size)\n",
    "            \n",
    "            inner_optimizer = deepcopy(self.outer_optimizer)\n",
    "            fast_model.train()\n",
    "            \n",
    "            print('----Task',task_id, '----')\n",
    "            for i in range(0,num_inner_update_step):\n",
    "                all_loss = []\n",
    "                for inner_step, batch in enumerate(support_dataloader):\n",
    "                    \n",
    "                    batch = tuple(t.to(self.device) for t in batch)\n",
    "                    input_ids, attention_mask, segment_ids, label_id = batch\n",
    "     \n",
    "                    \n",
    "                    logits, hiden_state = fast_model(input_ids, attention_mask)\n",
    "\n",
    "\n",
    "                    #print(logits)\n",
    "                    #print(label_id)\n",
    "                    \n",
    "                    loss = self.loss_fn(logits, label_id)    \n",
    "                    #loss = outputs[0]              \n",
    "                    loss.backward()\n",
    "                    inner_optimizer.step()\n",
    "                    inner_optimizer.zero_grad()\n",
    "                    \n",
    "                    all_loss.append(loss.item())\n",
    "                \n",
    "                if i % 4 == 0:\n",
    "                    print(\"Inner Loss: \", np.mean(all_loss))\n",
    "\n",
    "            query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
    "            \n",
    "            #iter and next seperate\n",
    "            query_iter = iter(query_dataloader)\n",
    "            query_batch = next(query_iter)\n",
    "            query_batch = tuple(t.to(self.device) for t in query_batch)\n",
    "            q_input_ids, q_attention_mask, q_segment_ids, q_label_id = query_batch\n",
    "            logits, hiden_state = fast_model(q_input_ids, q_attention_mask)\n",
    "            \n",
    "            if training:\n",
    "                q_loss = self.loss_fn(logits, q_label_id)\n",
    "\n",
    "                q_loss.backward()\n",
    "                fast_model.to(torch.device('cpu'))\n",
    "\n",
    "                # None parameter layers are removed\n",
    "                for i, params in enumerate(fast_model.parameters()):\n",
    "                    if task_id == 0:\n",
    "                        sum_gradients.append(deepcopy(params.grad))\n",
    "                    else:\n",
    "                        if(params.grad == None ):\n",
    "                            pass\n",
    "                        else:\n",
    "                            sum_gradients[i] += deepcopy(params.grad)\n",
    "            \n",
    "            q_logits = F.softmax(logits,dim=1)\n",
    "            pre_label_id = torch.argmax(q_logits,dim=1)\n",
    "            pre_label_id = pre_label_id.detach().cpu().numpy().tolist()\n",
    "            q_label_id = q_label_id.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            acc = accuracy_score(pre_label_id,q_label_id)\n",
    "            task_accs.append(acc)\n",
    "\n",
    "            q_loss = self.loss_fn(logits, q_label_id)\n",
    "            task_loss.append(q_loss)\n",
    "            \n",
    "            del fast_model, inner_optimizer\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        if training:\n",
    "            # Average gradient across tasks\n",
    "            for i in range(0,len(sum_gradients)):\n",
    "                if(sum_gradients[i] == None ):\n",
    "                            pass\n",
    "                else:\n",
    "                    sum_gradients[i] = sum_gradients[i] / float(num_task)\n",
    "\n",
    "            #Assign gradient for original model, then using optimizer to update its weights\n",
    "            for i, params in enumerate(self.model.parameters()):\n",
    "                params.grad = sum_gradients[i]\n",
    "\n",
    "\n",
    "            self.outer_optimizer.step()\n",
    "            self.outer_optimizer.zero_grad()\n",
    "            \n",
    "            del sum_gradients\n",
    "            gc.collect()\n",
    "        \n",
    "        return np.mean(task_accs) , np.mean(task_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c608662-44bc-4449-84ba-33e09d0b3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "import collections\n",
    "import random\n",
    "import json, pickle\n",
    "\n",
    "class MetaTask(Dataset):\n",
    "    \n",
    "    def __init__(self, examples, num_task, k_support, k_query, tokenizer):\n",
    "        \"\"\"\n",
    "        :param samples: list of samples\n",
    "        :param num_task: number of training tasks.\n",
    "        :param k_support: number of support sample per task\n",
    "        :param k_query: number of query sample per task\n",
    "        \"\"\"\n",
    "        self.examples = examples\n",
    "        random.shuffle(self.examples)\n",
    "        \n",
    "        self.num_task = num_task\n",
    "        self.k_support = k_support\n",
    "        self.k_query = k_query\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = 256\n",
    "        self.create_batch(self.num_task)\n",
    "    \n",
    "    def create_batch(self, num_task):\n",
    "        self.supports = []  # support set\n",
    "        self.queries = []  # query set\n",
    "        \n",
    "        for b in range(num_task):  # for each task\n",
    "            # 1.select domain randomly\n",
    "            domain = random.choice(self.examples)['label']\n",
    "            domainExamples = [e for e in self.examples if e['label'] == domain]\n",
    "            \n",
    "            # 1.select k_support + k_query examples from domain randomly\n",
    "            selected_examples = random.sample(domainExamples,self.k_support + self.k_query)\n",
    "            random.shuffle(selected_examples)\n",
    "            exam_train = selected_examples[:self.k_support]\n",
    "            exam_test  = selected_examples[self.k_support:]\n",
    "            \n",
    "            self.supports.append(exam_train)\n",
    "            self.queries.append(exam_test)\n",
    "\n",
    "    def create_feature_set(self,examples):\n",
    "        all_input_ids      = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
    "        all_attention_mask = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
    "        all_segment_ids    = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
    "        all_label_ids      = torch.empty(len(examples), dtype = torch.long)\n",
    "\n",
    "        for id_,example in enumerate(examples):\n",
    "            input_ids = self.tokenizer.encode(example['sentence'])\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            segment_ids    = [0] * len(input_ids)\n",
    "\n",
    "            while len(input_ids) < self.max_seq_length:\n",
    "                input_ids.append(0)\n",
    "                attention_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            label_id = example['label']\n",
    "\n",
    "            \n",
    "            all_input_ids[id_] = torch.Tensor(input_ids).to(torch.long)\n",
    "            all_attention_mask[id_] = torch.Tensor(attention_mask).to(torch.long)\n",
    "            all_segment_ids[id_] = torch.Tensor(segment_ids).to(torch.long)\n",
    "            all_label_ids[id_] = torch.Tensor([label_id]).to(torch.long)\n",
    "\n",
    "        tensor_set = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)  \n",
    "        return tensor_set\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        support_set = self.create_feature_set(self.supports[index])\n",
    "        query_set   = self.create_feature_set(self.queries[index])\n",
    "        return support_set, query_set\n",
    "\n",
    "    def __len__(self):\n",
    "        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n",
    "        return self.num_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54e2d8ca-437a-4358-b135-73138b099434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "def random_seed(value):\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed(value)\n",
    "    np.random.seed(value)\n",
    "    random.seed(value)\n",
    "\n",
    "def create_batch_of_tasks(taskset, is_shuffle = True, batch_size = 4):\n",
    "    idxs = list(range(0,len(taskset)))\n",
    "    if is_shuffle:\n",
    "        random.shuffle(idxs)\n",
    "    for i in range(0,len(idxs), batch_size):\n",
    "        yield [taskset[idxs[i]] for i in range(i, min(i + batch_size,len(taskset)))]\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    with torch.no_grad():\n",
    "            logits, _ = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        predict += preds.tolist()\n",
    "        y_true += b_labels.tolist()\n",
    "\n",
    "    # plot heatmap\n",
    "    test_accuracy = np.mean(test_accuracy)\n",
    "    cm = confusion_matrix(y_true, predict)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sn.heatmap(cm, annot=True)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy\n",
    "    print(f'Accuracy: {accuracy_score(y_true, predict)}')\n",
    "\n",
    "    # Recall\n",
    "    print(f'Recall: {recall_score(y_true, predict, average=None)}')\n",
    "\n",
    "    # Precision\n",
    "    print(f'Precision: {precision_score(y_true, predict, average=None)}')\n",
    "\n",
    "    # F1_score\n",
    "    print(f'F1_score: {f1_score(y_true, predict, average=None)}')\n",
    "\n",
    "    return accuracy_score(y_true, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8253e4-aeea-4d6b-aa8c-7e416d16fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def test_evaluate(model,model_path, test_dataloader,hidden=16,num_labels=2,feature_remove_max=False):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our vtest set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    if(model == QModel_Classifier):\n",
    "        model = model(1024,hidden, num_labels=num_classes, dropout=0.1,feature_remove_max=feature_remove_max)\n",
    "    else:\n",
    "        model = model(1024,hidden, num_labels=num_classes, dropout=0.1)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    test_accuracy = []\n",
    "    predict = []\n",
    "    y_true = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f766a09b-a7e2-4c56-bdb6-b0f639b9de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data, MAX_LEN=256):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,  # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,  # Pad sentence to max length\n",
    "            return_attention_mask=True  # Return attention mask\n",
    "        )\n",
    "\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "def prepare_data(test_path, tsv = False):\n",
    "    if(tsv == True) : \n",
    "        test_df = pd.read_csv(train_path, sep='\\t')\n",
    "\n",
    "    else:\n",
    "        test_df = pd.read_csv(train_path)\n",
    "\n",
    "    test_text = test_df[\"sentence\"]\n",
    "    test_label = test_df[\"label\"]\n",
    "    for i in range(len(test_text)):\n",
    "        test_text[i] = text_preprocessing(test_text[i])\n",
    "        \n",
    "    test_inputs, test_masks = preprocessing_for_bert(test_text, MAX_LEN)\n",
    "    test_labels = torch.tensor(test_label)\n",
    "    \n",
    "    batch_size = 64\n",
    "    test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c833735e-e762-43c3-a99f-d2bf333ffdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/Cellar/jupyterlab/4.0.6_1/libexec/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Task 0 ----\n",
      "Inner Loss:  2.445026397705078\n",
      "Inner Loss:  2.431630849838257\n",
      "Inner Loss:  2.4546172618865967\n",
      "----Task 1 ----\n",
      "Inner Loss:  2.297107219696045\n",
      "Inner Loss:  2.3064944744110107\n",
      "Inner Loss:  2.2736544609069824\n",
      "Step: 0 \ttraining Acc: 0.0\n",
      "\n",
      "-----------------Testing Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.2386443614959717\n",
      "Inner Loss:  2.227292537689209\n",
      "Inner Loss:  2.184762954711914\n",
      "Inner Loss:  2.333995819091797\n",
      "Inner Loss:  2.3043158054351807\n",
      "Inner Loss:  2.3041958808898926\n",
      "Inner Loss:  2.2687318325042725\n",
      "Inner Loss:  2.277085542678833\n",
      "Inner Loss:  2.250636577606201\n",
      "Inner Loss:  2.2154786586761475\n",
      "----Task 0 ----\n",
      "Inner Loss:  3.4664204120635986\n",
      "Inner Loss:  3.428532123565674\n",
      "Inner Loss:  3.4227397441864014\n",
      "Inner Loss:  3.4421677589416504\n",
      "Inner Loss:  3.46010160446167\n",
      "Inner Loss:  3.4243366718292236\n",
      "Inner Loss:  3.4100594520568848\n",
      "Inner Loss:  3.4320316314697266\n",
      "Inner Loss:  3.5647244453430176\n",
      "Inner Loss:  3.4286227226257324\n",
      "----Task 0 ----\n",
      "Inner Loss:  3.424633026123047\n",
      "Inner Loss:  3.454510450363159\n",
      "Inner Loss:  3.419282913208008\n",
      "Inner Loss:  3.3982555866241455\n",
      "Inner Loss:  3.440101146697998\n",
      "Inner Loss:  3.3794643878936768\n",
      "Inner Loss:  3.4405643939971924\n",
      "Inner Loss:  3.4256653785705566\n",
      "Inner Loss:  3.468674898147583\n",
      "Inner Loss:  3.4193332195281982\n",
      "Step: 0 Test F1: 0.0\n",
      "----Task 0 ----\n",
      "Inner Loss:  3.642976760864258\n",
      "Inner Loss:  3.6549594402313232\n",
      "Inner Loss:  3.6627070903778076\n",
      "----Task 1 ----\n",
      "Inner Loss:  2.2493667602539062\n",
      "Inner Loss:  2.2286975383758545\n",
      "Inner Loss:  2.213373899459839\n",
      "Step: 1 \ttraining Acc: 0.0\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.6824116706848145\n",
      "Inner Loss:  2.6867516040802\n",
      "Inner Loss:  2.67648983001709\n",
      "----Task 1 ----\n",
      "Inner Loss:  2.292248487472534\n",
      "Inner Loss:  2.2489309310913086\n",
      "Inner Loss:  2.323477268218994\n",
      "Step: 2 \ttraining Acc: 0.0\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.282188653945923\n",
      "Inner Loss:  2.2230231761932373\n",
      "Inner Loss:  2.2593607902526855\n",
      "----Task 1 ----\n",
      "Inner Loss:  3.636904239654541\n",
      "Inner Loss:  3.6024997234344482\n",
      "Inner Loss:  3.5961074829101562\n",
      "Step: 3 \ttraining Acc: 0.0\n",
      "----Task 0 ----\n",
      "Inner Loss:  3.5389304161071777\n",
      "Inner Loss:  3.424042224884033\n",
      "Inner Loss:  3.584683895111084\n",
      "----Task 1 ----\n",
      "Inner Loss:  2.21459698677063\n",
      "Inner Loss:  2.1925647258758545\n",
      "Inner Loss:  2.1915974617004395\n",
      "Step: 4 \ttraining Acc: 0.5\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.0432686805725098\n",
      "Inner Loss:  2.0386645793914795\n",
      "Inner Loss:  2.009488344192505\n",
      "----Task 1 ----\n",
      "Inner Loss:  3.4588191509246826\n",
      "Inner Loss:  3.526068925857544\n",
      "Inner Loss:  3.468590497970581\n",
      "Step: 5 \ttraining Acc: 0.25\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.007286787033081\n",
      "Inner Loss:  2.0487403869628906\n",
      "Inner Loss:  1.9188244342803955\n",
      "----Task 1 ----\n",
      "Inner Loss:  3.4368369579315186\n",
      "Inner Loss:  3.5092499256134033\n",
      "Inner Loss:  3.4114222526550293\n",
      "Step: 6 \ttraining Acc: 0.25\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.7572208642959595\n",
      "Inner Loss:  1.6584231853485107\n",
      "Inner Loss:  2.0145461559295654\n",
      "----Task 1 ----\n",
      "Inner Loss:  3.395399808883667\n",
      "Inner Loss:  3.378175735473633\n",
      "Inner Loss:  3.388526201248169\n",
      "Step: 7 \ttraining Acc: 0.25\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.7576918601989746\n",
      "Inner Loss:  2.662675380706787\n",
      "Inner Loss:  2.598306655883789\n",
      "----Task 1 ----\n",
      "Inner Loss:  3.3289241790771484\n",
      "Inner Loss:  3.1760034561157227\n",
      "Inner Loss:  3.334965229034424\n",
      "Step: 8 \ttraining Acc: 0.0\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.5880961418151855\n",
      "Inner Loss:  2.650029420852661\n",
      "Inner Loss:  2.5883796215057373\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.929534912109375\n",
      "Inner Loss:  1.7461093664169312\n",
      "Inner Loss:  1.915165662765503\n",
      "Step: 9 \ttraining Acc: 0.25\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.5756659507751465\n",
      "Inner Loss:  2.545681953430176\n",
      "Inner Loss:  2.584505558013916\n",
      "----Task 1 ----\n",
      "Inner Loss:  2.6233227252960205\n",
      "Inner Loss:  2.7619452476501465\n",
      "Inner Loss:  2.6526951789855957\n",
      "Step: 10 \ttraining Acc: 0.0\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.6168934106826782\n",
      "Inner Loss:  1.5118916034698486\n",
      "Inner Loss:  1.6168272495269775\n",
      "----Task 1 ----\n",
      "Inner Loss:  2.8467612266540527\n",
      "Inner Loss:  2.9432032108306885\n",
      "Inner Loss:  3.0435245037078857\n",
      "Step: 11 \ttraining Acc: 0.25\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.771564245223999\n",
      "Inner Loss:  1.6895816326141357\n",
      "Inner Loss:  1.6221978664398193\n",
      "----Task 1 ----\n",
      "Inner Loss:  2.829566240310669\n",
      "Inner Loss:  3.0348927974700928\n",
      "Inner Loss:  2.8340444564819336\n",
      "Step: 12 \ttraining Acc: 0.5\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.7096896171569824\n",
      "Inner Loss:  2.555347442626953\n",
      "Inner Loss:  2.6644814014434814\n",
      "----Task 1 ----\n",
      "Inner Loss:  3.1001951694488525\n",
      "Inner Loss:  2.960362195968628\n",
      "Inner Loss:  3.073707103729248\n",
      "Step: 13 \ttraining Acc: 0.0\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.593750238418579\n",
      "Inner Loss:  1.889286756515503\n",
      "Inner Loss:  1.6753759384155273\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.6142499446868896\n",
      "Inner Loss:  1.8232080936431885\n",
      "Inner Loss:  1.833264946937561\n",
      "Step: 14 \ttraining Acc: 0.75\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.71376371383667\n",
      "Inner Loss:  2.5360329151153564\n",
      "Inner Loss:  2.8777995109558105\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.5941760540008545\n",
      "Inner Loss:  1.4795500040054321\n",
      "Inner Loss:  1.5227457284927368\n",
      "Step: 15 \ttraining Acc: 0.5\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.8126510381698608\n",
      "Inner Loss:  1.589436411857605\n",
      "Inner Loss:  1.963104248046875\n",
      "----Task 1 ----\n",
      "Inner Loss:  2.43318510055542\n",
      "Inner Loss:  2.3834621906280518\n",
      "Inner Loss:  2.6218178272247314\n",
      "Step: 16 \ttraining Acc: 0.0\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.086610794067383\n",
      "Inner Loss:  1.9170929193496704\n",
      "Inner Loss:  1.8363126516342163\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.9931669235229492\n",
      "Inner Loss:  2.033691883087158\n",
      "Inner Loss:  2.0915133953094482\n",
      "Step: 17 \ttraining Acc: 0.0\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.681146264076233\n",
      "Inner Loss:  1.5502901077270508\n",
      "Inner Loss:  1.470012903213501\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.6042124032974243\n",
      "Inner Loss:  2.1130247116088867\n",
      "Inner Loss:  1.8543987274169922\n",
      "Step: 18 \ttraining Acc: 0.75\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.0843744277954102\n",
      "Inner Loss:  1.5391417741775513\n",
      "Inner Loss:  1.2444874048233032\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.4235608577728271\n",
      "Inner Loss:  1.7243187427520752\n",
      "Inner Loss:  1.702035903930664\n",
      "Step: 19 \ttraining Acc: 0.5\n",
      "----Task 0 ----\n",
      "Inner Loss:  3.5159218311309814\n",
      "Inner Loss:  4.290163516998291\n",
      "Inner Loss:  3.791309356689453\n",
      "----Task 1 ----\n",
      "Inner Loss:  3.8384711742401123\n",
      "Inner Loss:  4.378739356994629\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AdamW\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "def main():\n",
    "    args = config()\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(args.bert_model, do_lower_case = True)\n",
    "    tsv = False\n",
    "    best_validation_loss = float('inf')\n",
    "    model_path = r\"fewshot_qmodel.pt\"\n",
    "    \n",
    "    train_path = \"trec/train.tsv\"\n",
    "    valid_path = \"trec/dev.tsv\"\n",
    "    test_path = 'trec/test.tsv'\n",
    "\n",
    "    if(train_path[-3:]=='tsv'):\n",
    "        tsv = True\n",
    "    \n",
    "    if(tsv == True) : \n",
    "        train_ds = pd.read_csv(train_path, sep='\\t')\n",
    "        val_ds = pd.read_csv(valid_path, sep='\\t')\n",
    "    else:\n",
    "        train_ds = pd.read_csv(train_path)\n",
    "        val_ds = pd.read_csv(valid_path)\n",
    "\n",
    "    args.num_labels = len(val_ds.label.unique())\n",
    "\n",
    "    #initialize model \n",
    "    learner = Learner(args)\n",
    "\n",
    "    \n",
    "    test_examples = []\n",
    "\n",
    "    for index, row in list(val_ds.iterrows()):\n",
    "        test_examples.append(dict(row))\n",
    "\n",
    "\n",
    "    train_examples = []\n",
    "\n",
    "    for index, row in list(train_ds.iterrows()):\n",
    "        train_examples.append(dict(row))\n",
    "        \n",
    "    test = MetaTask(test_examples, num_task = args.num_task_test, k_support=args.k_spt, \n",
    "                    k_query=args.k_qry, tokenizer = tokenizer)\n",
    "    \n",
    "    global_step = 0\n",
    "    for epoch in range(args.epoch):\n",
    "    \n",
    "        train = MetaTask(train_examples, num_task = args.num_task_train, k_support=args.k_spt, \n",
    "                         k_query=args.k_qry, tokenizer = tokenizer)\n",
    "    \n",
    "        db = create_batch_of_tasks(train, is_shuffle = True, batch_size = args.outer_batch_size)\n",
    "    \n",
    "        for step, task_batch in enumerate(db):\n",
    "    \n",
    "            acc,_ = learner(task_batch)\n",
    "    \n",
    "            print('Step:', step, '\\ttraining Acc:', acc)\n",
    "    \n",
    "            if global_step % 20 == 0:\n",
    "                random_seed(123)\n",
    "                print(\"\\n-----------------Testing Mode-----------------\\n\")\n",
    "                db_test = create_batch_of_tasks(test, is_shuffle = False, batch_size = 1)\n",
    "                acc_all_test = []\n",
    "                loss_all_test = []\n",
    "    \n",
    "                for test_batch in db_test:\n",
    "                    acc,loss = learner(test_batch, training = False)\n",
    "                    acc_all_test.append(acc)\n",
    "                    loss_all_test.append(loss)\n",
    "                    \n",
    "                print('Step:', step, 'Test F1:', np.mean(acc_all_test))\n",
    "    \n",
    "                random_seed(int(time.time() % 10))\n",
    "                \n",
    "                val_loss = np.mean(acc_all_test)\n",
    "                if (val_loss < best_validation_loss):\n",
    "                best_validation_loss = val_loss\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                \n",
    "            global_step += 1\n",
    "\n",
    "    test_dataloader = prepare_data(test_path, tsv = tsv)\n",
    "    test_accuracy = test_evaluate(QModel_Classifier,model_path, test_dataloader,hidden=16,num_labels=args.num_labels)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3838298-e762-4da8-9478-54c33f02b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "model , outer_optimizer = initialize_model(Model_Classifier,1024,16,6)\n",
    "for i, params in enumerate(model.parameters()):\n",
    "    print(i)\n",
    "    print(params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e6d32-c7df-404a-9cf6-7a2c329d279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.linear.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce9542d-0b97-438c-8938-13a59c04a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaModel.from_pretrained('roberta-large')\n",
    "for i, params in enumerate(model.parameters()):\n",
    "\n",
    "    print(i)\n",
    "    print(params.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
